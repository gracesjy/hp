---
title: LLM+LoRA+RAG+..
author: Raymond
date: 2025-09-30
category: Jekyll
layout: post
---

## LoRA
Meta-Llama-3-8B-Instruct 모델을 LoRA 방식으로 학습하고 Ollama에서 사용할 수 있도록 변환하는 과정은 다음과 같은 단계로 진행할 수 있습니다. 이전에 DeepSeek 모델에서 실패하셨던 부분을 고려해, 이번에는 LLaMA 3 모델에 맞춘 정확한 절차를 ..

---

🧠 1. LoRA로 Meta-Llama-3-8B-Instruct 파인튜닝하기

준비 사항
- Hugging Face에서 Meta-Llama-3-8B-Instruct 모델에 접근 권한을 신청해야 합니다.
- 최신 버전의 transformers, peft, accelerate, datasets 라이브러리를 설치하세요:
  `bash
  pip install -U transformers peft accelerate datasets
  `

데이터셋 포맷
- 데이터는 아래와 같은 형식의 JSON 또는 CSV로 준비합니다:
  `json
  {
    "question": "프롬프트 내용",
    "answer": "모델이 생성해야 할 응답"
  }
  `

LoRA 학습 코드 예시
- Hugging Face의 PEFT 라이브러리를 사용하여 LoRA 방식으로 학습합니다.
- SFTTrainer 또는 Trainer를 사용할 수 있으며, SFTTrainer는 적은 데이터셋에 적합합니다.

자세한 튜토리얼은 이 블로그에서 확인할 수 있습니다.

---

🔄 2. Ollama에서 사용 가능하도록 변환하기

Ollama는 자체 포맷(.gguf)을 사용하므로, 학습된 LoRA 모델을 Ollama에서 사용하려면 다음 단계를 거쳐야 합니다:

① LoRA 병합
- 학습된 LoRA 어댑터를 원본 LLaMA 3 모델에 병합합니다:
  `python
  from peft import PeftModel
  from transformers import AutoModelForCausalLM

  basemodel = AutoModelForCausalLM.frompretrained("meta-llama/Meta-Llama-3-8B-Instruct")
  loramodel = PeftModel.frompretrained(basemodel, "path/to/loraadapter")
  mergedmodel = loramodel.mergeandunload()
  mergedmodel.savepretrained("merged-llama3")
  `

② GGUF 포맷으로 변환
- 병합된 모델을 llama.cpp 또는 transformers-to-gguf 도구를 사용해 .gguf 포맷으로 변환합니다.
- 예시:
  `bash
  python3 convert.py --modelpath merged-llama3 --outputfile llama3-merged.gguf
  `

③ Ollama에 등록
- 변환된 .gguf 파일을 Ollama의 모델 디렉토리에 넣고 Modelfile을 작성합니다:
  `
  FROM llama3-merged.gguf
  NAME my-llama3-lora
  `

- 이후 ollama run my-llama3-lora로 실행 가능합니다.

---

💡 팁
- LoRA 학습 시 r, alpha, dropout 등의 하이퍼파라미터를 조정하면 성능에 큰 영향을 줄 수 있습니다.
- Ollama는 quantized gguf 모델을 선호하므로, 변환 시 q4KM, q5KM 등의 양자화 옵션을 고려해보세요.

---

진영님, 혹시 이전 DeepSeek 모델에서 실패했던 부분이 어떤 단계였는지 알려주시면, 그에 맞춰 더 구체적인 해결책도 드릴 수 있어요. 또는 지금 LLaMA 3 모델로 학습을 시작하시려면, 데이터셋 포맷이나 학습 스크립트부터 함께 준비해볼까요?