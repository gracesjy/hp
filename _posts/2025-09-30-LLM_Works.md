---
title: LLM+LoRA+RAG+..
author: Raymond
date: 2025-09-30
category: Jekyll
layout: post
---

## LoRA
The process of training the Meta-Llama-3-8B-Instruct model in the LoRA manner and converting it for use on Ollama can be done in the following steps. Considering the previous failures with the DeepSeek model, this time we will follow the exact procedure tailored to the LLaMA 3 model.<br>

---

ðŸ§  1. Fine-tuning Meta-Llama-3-8B-Instruct with LoRA

Prerequisites 
- You must request access to the Meta-Llama-3-8B-Instruct model from Hugging Face. 
- Download
```
git lfs install
git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
```
- Download transformers, peft, accelerate, datasets libraries:
  `bash
  pip install -U transformers peft accelerate datasets
  `

Dataset Preparations
- Prepare the data as JSON or CSV in the following format:
  `json
  {
    "question": "Prompt Contents",
    "answer": "Answer generated by Model"
  }
  `

LoRA Training Code Example
- Train by Hugging Face's PEFT library
- SFTTrainer or Trainer, SFTTrainer fits for small dataset.

---
ðŸ§  2. Convert  to be available in Ollama


Since Ollama uses its own format (.gguf), the following steps are required to use the trained LoRA model in Ollama:

â‘  LoRA Merge
- Merge the trained LoRA adapter into the original LLaMA 3 model:
  `python
  from peft import PeftModel
  from transformers import AutoModelForCausalLM

  basemodel = AutoModelForCausalLM.frompretrained("meta-llama/Meta-Llama-3-8B-Instruct")
  loramodel = PeftModel.frompretrained(basemodel, "path/to/loraadapter")
  mergedmodel = loramodel.merge_and_unload()
  mergedmodel.save_pretrained("merged-llama3")
  `

â‘¡ Convert to GGUF format
- Convert the merged model to .gguf format using llama.cpp or the transformers-to-gguf tool.
- Example:
  `bash
  python3 convert.py --modelpath merged-llama3 --outputfile llama3-merged.gguf
  `

â‘¢ Register Ollama
- Place the converted .gguf file in Ollama's model directory and create a Modelfile:
  `
  FROM llama3-merged.gguf
  NAME my-llama3-lora
  `

- After then, ollama run my-llama3-lora

---

ðŸ’¡ Tips
- When training LoRA, adjusting hyperparameters such as r, alpha, and dropout can have a significant impact on performance.
- Ollama prefers quantized gguf models, so consider quantization options like q4KM or q5KM when converting. It seems like you should be able to fit the VRAM size of your GPU at home.

---

## LoRA Training

```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm

# Model Path in your PC or ..
model_id = "./Meta-Llama-3-8B-Instruct"

# Load Tokeninzer
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# Load Model, 
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA Setup
peft_config = LoraConfig(
    r=4,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# LoRA Apply
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# Load Dataset
dataset = load_dataset("json", data_files="train.jsonl")

# Preprocessing Function 
def format_example(example):
    inputs = tokenizer(
        example["instruction"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    labels = tokenizer(
        example["output"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    return {
        "input_ids": inputs["input_ids"][0],
        "labels": labels["input_ids"][0]
    }

tokenized_ds = dataset["train"].map(format_example)
dataloader = DataLoader(tokenized_ds, batch_size=1)

# Setup Optimizer
optimizer = AdamW(model.parameters(), lr=1e-4)

# Train Loop
model.train()
for epoch in range(1):  # Fast ..
    for batch in tqdm(dataloader):
        input_ids = batch["input_ids"].unsqueeze(0).to(model.device)
        labels = batch["labels"].unsqueeze(0).to(model.device)

        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# Save LoRA Adapter
model.save_pretrained("./lora-llama3")
tokenizer.save_pretrained("./lora-llama3")
```
In case of device_map error -> .to("cuda")
### Merge

```
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load Base Model
base_model = AutoModelForCausalLM.from_pretrained(
    "./Meta-Llama-3-8B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA Adapter
lora_model = PeftModel.from_pretrained(base_model, "./lora-llama3")

# Merge
merged_model = lora_model.merge_and_unload()

# Save
merged_model.save_pretrained("./merged-llama3", safe_serialization=False)
tokenizer = AutoTokenizer.from_pretrained("./Meta-Llama-3-8B-Instruct")
tokenizer.save_pretrained("./merged-llama3")
```
### Verify Fine Tunning Model

```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Setup
BASE_MODEL = "meta-llama/Meta-Llama-3-8B-Instruct"
LORA_PATH = "./lora_llama3"   # í•™ìŠµ ì‹œ ì €ìž¥í•œ LoRA adapter ê²½ë¡œ
PROMPT = "You Question .. "

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

# === Base Model ===
print("ðŸš€Base Model Load ...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    torch_dtype=torch.float16
)
base_model.eval()

# === Fine Tuning Model Load ===
print("ðŸš€ LoRA Applided Model ..")
lora_model = PeftModel.from_pretrained(
    base_model,
    LORA_PATH
)
lora_model.eval()

# === Input ===
inputs = tokenizer(PROMPT, return_tensors="pt").to(lora_model.device)

# === Base Model Inference ===
with torch.no_grad():
    base_out = base_model.generate(**inputs, max_new_tokens=200)
base_text = tokenizer.decode(base_out[0], skip_special_tokens=True)

# === Fine Tuning Model Inference ===
with torch.no_grad():
    lora_out = lora_model.generate(**inputs, max_new_tokens=200)
lora_text = tokenizer.decode(lora_out[0], skip_special_tokens=True)

# === Output ===
print("\n==============================")
print("ðŸ“Œ Prompt:")
print(PROMPT)
print("\n--- Base Model ---")
print(base_text)
print("\n--- LoRA Model ---")
print(lora_text)
print("==============================")
```

If you accept above results, run belows.

### Ollama model
```
echo 'FROM ./gemma-2-9b-it-Q6_K_L.gguf' > Modelfile
ollama create gemma-2-9b-it -f Modelfile
ollama list

ollama server
ollama run gemma-2-9b-it
```
