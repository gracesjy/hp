---
title: LLM+LoRA+RAG+..
author: Raymond
date: 2025-09-30
category: Jekyll
layout: post
---

## LoRA
Meta-Llama-3-8B-Instruct ëª¨ë¸ì„ LoRA ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê³  Ollamaì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ ë³€í™˜í•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ì§„í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ì „ì— DeepSeek ëª¨ë¸ì—ì„œ ì‹¤íŒ¨í•˜ì…¨ë˜ ë¶€ë¶„ì„ ê³ ë ¤í•´, ì´ë²ˆì—ëŠ” LLaMA 3 ëª¨ë¸ì— ë§žì¶˜ ì •í™•í•œ ì ˆì°¨ë¥¼ ..

---

ðŸ§  1. LoRAë¡œ Meta-Llama-3-8B-Instruct íŒŒì¸íŠœë‹í•˜ê¸°

ì¤€ë¹„ ì‚¬í•­
- Hugging Faceì—ì„œ Meta-Llama-3-8B-Instruct ëª¨ë¸ì— ì ‘ê·¼ ê¶Œí•œì„ ì‹ ì²­í•´ì•¼ í•©ë‹ˆë‹¤.
- ë‹¤ìš´ë¡œë“œ
```
git lfs install
git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
```
- ìµœì‹  ë²„ì „ì˜ transformers, peft, accelerate, datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:
  `bash
  pip install -U transformers peft accelerate datasets
  `

ë°ì´í„°ì…‹ í¬ë§·
- ë°ì´í„°ëŠ” ì•„ëž˜ì™€ ê°™ì€ í˜•ì‹ì˜ JSON ë˜ëŠ” CSVë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤:
  `json
  {
    "question": "í”„ë¡¬í”„íŠ¸ ë‚´ìš©",
    "answer": "ëª¨ë¸ì´ ìƒì„±í•´ì•¼ í•  ì‘ë‹µ"
  }
  `

LoRA í•™ìŠµ ì½”ë“œ ì˜ˆì‹œ
- Hugging Faceì˜ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ LoRA ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
- SFTTrainer ë˜ëŠ” Trainerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆìœ¼ë©°, SFTTrainerëŠ” ì ì€ ë°ì´í„°ì…‹ì— ì í•©í•©ë‹ˆë‹¤.

ìžì„¸í•œ íŠœí† ë¦¬ì–¼ì€ ì´ ë¸”ë¡œê·¸ì—ì„œ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

---

ðŸ”„ 2. Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜í•˜ê¸°

OllamaëŠ” ìžì²´ í¬ë§·(.gguf)ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, í•™ìŠµëœ LoRA ëª¨ë¸ì„ Ollamaì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê±°ì³ì•¼ í•©ë‹ˆë‹¤:

â‘  LoRA ë³‘í•©
- í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ LLaMA 3 ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤:
  `python
  from peft import PeftModel
  from transformers import AutoModelForCausalLM

  basemodel = AutoModelForCausalLM.frompretrained("meta-llama/Meta-Llama-3-8B-Instruct")
  loramodel = PeftModel.frompretrained(basemodel, "path/to/loraadapter")
  mergedmodel = loramodel.mergeandunload()
  mergedmodel.savepretrained("merged-llama3")
  `

â‘¡ GGUF í¬ë§·ìœ¼ë¡œ ë³€í™˜
- ë³‘í•©ëœ ëª¨ë¸ì„ llama.cpp ë˜ëŠ” transformers-to-gguf ë„êµ¬ë¥¼ ì‚¬ìš©í•´ .gguf í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- ì˜ˆì‹œ:
  `bash
  python3 convert.py --modelpath merged-llama3 --outputfile llama3-merged.gguf
  `

â‘¢ Ollamaì— ë“±ë¡
- ë³€í™˜ëœ .gguf íŒŒì¼ì„ Ollamaì˜ ëª¨ë¸ ë””ë ‰í† ë¦¬ì— ë„£ê³  Modelfileì„ ìž‘ì„±í•©ë‹ˆë‹¤:
  `
  FROM llama3-merged.gguf
  NAME my-llama3-lora
  `

- ì´í›„ ollama run my-llama3-loraë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

ðŸ’¡ íŒ
- LoRA í•™ìŠµ ì‹œ r, alpha, dropout ë“±ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ë©´ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì¤„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- OllamaëŠ” quantized gguf ëª¨ë¸ì„ ì„ í˜¸í•˜ë¯€ë¡œ, ë³€í™˜ ì‹œ q4KM, q5KM ë“±ì˜ ì–‘ìží™” ì˜µì…˜ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.

---

í˜¹ì‹œ ì´ì „ DeepSeek ëª¨ë¸ì—ì„œ ì‹¤íŒ¨í–ˆë˜ ë¶€ë¶„ì´ ì–´ë–¤ ë‹¨ê³„ì˜€ëŠ”ì§€ ì•Œë ¤ì£¼ì‹œë©´, ê·¸ì— ë§žì¶° ë” êµ¬ì²´ì ì¸ í•´ê²°ì±…ë„ ë“œë¦´ ìˆ˜ ìžˆì–´ìš”. ë˜ëŠ” ì§€ê¸ˆ LLaMA 3 ëª¨ë¸ë¡œ í•™ìŠµì„ ì‹œìž‘í•˜ì‹œë ¤ë©´, ë°ì´í„°ì…‹ í¬ë§·ì´ë‚˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ë¶€í„° í•¨ê»˜ ì¤€ë¹„í•´ë³¼ê¹Œìš”?

## LoRA Training

```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm

# ëª¨ë¸ ê²½ë¡œ
model_id = "./Meta-Llama-3-8B-Instruct"

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# ëª¨ë¸ ë¡œë“œ (ìžë™ ë¶„ì‚° + ë©”ëª¨ë¦¬ ìµœì í™”)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA ì„¤ì •
peft_config = LoraConfig(
    r=4,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# LoRA ì ìš©
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("json", data_files="train.jsonl")

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def format_example(example):
    inputs = tokenizer(
        example["instruction"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    labels = tokenizer(
        example["output"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    return {
        "input_ids": inputs["input_ids"][0],
        "labels": labels["input_ids"][0]
    }

tokenized_ds = dataset["train"].map(format_example)
dataloader = DataLoader(tokenized_ds, batch_size=1)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = AdamW(model.parameters(), lr=1e-4)

# í•™ìŠµ ë£¨í”„
model.train()
for epoch in range(1):  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
    for batch in tqdm(dataloader):
        input_ids = batch["input_ids"].unsqueeze(0).to(model.device)
        labels = batch["labels"].unsqueeze(0).to(model.device)

        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# LoRA ì–´ëŒ‘í„° ì €ìž¥
model.save_pretrained("./lora-llama3")
tokenizer.save_pretrained("./lora-llama3")
```
device_map ì˜¤ë¥˜ -> .to("cuda")
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm

# ëª¨ë¸ ê²½ë¡œ
model_id = "./Meta-Llama-3-8B-Instruct"

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# ëª¨ë¸ ë¡œë“œ (ìžë™ ë¶„ì‚° + ë©”ëª¨ë¦¬ ìµœì í™”)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA ì„¤ì •
peft_config = LoraConfig(
    r=4,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# LoRA ì ìš©
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("json", data_files="train.jsonl")

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def format_example(example):
    inputs = tokenizer(
        example["instruction"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    labels = tokenizer(
        example["output"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    return {
        "input_ids": inputs["input_ids"][0],
        "labels": labels["input_ids"][0]
    }

tokenized_ds = dataset["train"].map(format_example)
dataloader = DataLoader(tokenized_ds, batch_size=1)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = AdamW(model.parameters(), lr=1e-4)

# í•™ìŠµ ë£¨í”„
model.train()
for epoch in range(1):  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
    for batch in tqdm(dataloader):
        input_ids = batch["input_ids"].unsqueeze(0).to(model.device)
        labels = batch["labels"].unsqueeze(0).to(model.device)

        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# LoRA ì–´ëŒ‘í„° ì €ìž¥
model.save_pretrained("./lora-llama3")
tokenizer.save_pretrained("./lora-llama3")
```

### Merge

```
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# base ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    "./Meta-Llama-3-8B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA ì–´ëŒ‘í„° ë¡œë“œ
lora_model = PeftModel.from_pretrained(base_model, "./lora-llama3")

# ë³‘í•©
merged_model = lora_model.merge_and_unload()

# ì €ìž¥
merged_model.save_pretrained("./merged-llama3", safe_serialization=False)
tokenizer = AutoTokenizer.from_pretrained("./Meta-Llama-3-8B-Instruct")
tokenizer.save_pretrained("./merged-llama3")
```

### Ollama model
```
echo 'FROM ./gemma-2-9b-it-Q6_K_L.gguf' > Modelfile
ollama create gemma-2-9b-it -f Modelfile
ollama list

ollama server
ollama run gemma-2-9b-it
```
