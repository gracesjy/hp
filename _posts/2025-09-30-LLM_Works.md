---
title: LLM+LoRA+RAG+..
author: Raymond
date: 2025-09-30
category: Jekyll
layout: post
---

## LoRA
Meta-Llama-3-8B-Instruct 모델을 LoRA 방식으로 학습하고 Ollama에서 사용할 수 있도록 변환하는 과정은 다음과 같은 단계로 진행할 수 있습니다. 이전에 DeepSeek 모델에서 실패하셨던 부분을 고려해, 이번에는 LLaMA 3 모델에 맞춘 정확한 절차를 ..

---

🧠 1. LoRA로 Meta-Llama-3-8B-Instruct 파인튜닝하기

준비 사항
- Hugging Face에서 Meta-Llama-3-8B-Instruct 모델에 접근 권한을 신청해야 합니다.
- 다운로드
```
git lfs install
git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
```
- 최신 버전의 transformers, peft, accelerate, datasets 라이브러리를 설치하세요:
  `bash
  pip install -U transformers peft accelerate datasets
  `

데이터셋 포맷
- 데이터는 아래와 같은 형식의 JSON 또는 CSV로 준비합니다:
  `json
  {
    "question": "프롬프트 내용",
    "answer": "모델이 생성해야 할 응답"
  }
  `

LoRA 학습 코드 예시
- Hugging Face의 PEFT 라이브러리를 사용하여 LoRA 방식으로 학습합니다.
- SFTTrainer 또는 Trainer를 사용할 수 있으며, SFTTrainer는 적은 데이터셋에 적합합니다.

자세한 튜토리얼은 이 블로그에서 확인할 수 있습니다.

---

🔄 2. Ollama에서 사용 가능하도록 변환하기

Ollama는 자체 포맷(.gguf)을 사용하므로, 학습된 LoRA 모델을 Ollama에서 사용하려면 다음 단계를 거쳐야 합니다:

① LoRA 병합
- 학습된 LoRA 어댑터를 원본 LLaMA 3 모델에 병합합니다:
  `python
  from peft import PeftModel
  from transformers import AutoModelForCausalLM

  basemodel = AutoModelForCausalLM.frompretrained("meta-llama/Meta-Llama-3-8B-Instruct")
  loramodel = PeftModel.frompretrained(basemodel, "path/to/loraadapter")
  mergedmodel = loramodel.mergeandunload()
  mergedmodel.savepretrained("merged-llama3")
  `

② GGUF 포맷으로 변환
- 병합된 모델을 llama.cpp 또는 transformers-to-gguf 도구를 사용해 .gguf 포맷으로 변환합니다.
- 예시:
  `bash
  python3 convert.py --modelpath merged-llama3 --outputfile llama3-merged.gguf
  `

③ Ollama에 등록
- 변환된 .gguf 파일을 Ollama의 모델 디렉토리에 넣고 Modelfile을 작성합니다:
  `
  FROM llama3-merged.gguf
  NAME my-llama3-lora
  `

- 이후 ollama run my-llama3-lora로 실행 가능합니다.

---

💡 팁
- LoRA 학습 시 r, alpha, dropout 등의 하이퍼파라미터를 조정하면 성능에 큰 영향을 줄 수 있습니다.
- Ollama는 quantized gguf 모델을 선호하므로, 변환 시 q4KM, q5KM 등의 양자화 옵션을 고려해보세요.

---

혹시 이전 DeepSeek 모델에서 실패했던 부분이 어떤 단계였는지 알려주시면, 그에 맞춰 더 구체적인 해결책도 드릴 수 있어요. 또는 지금 LLaMA 3 모델로 학습을 시작하시려면, 데이터셋 포맷이나 학습 스크립트부터 함께 준비해볼까요?

## LoRA Training

```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm

# 모델 경로
model_id = "./Meta-Llama-3-8B-Instruct"

# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# 모델 로드 (자동 분산 + 메모리 최적화)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA 설정
peft_config = LoraConfig(
    r=4,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# LoRA 적용
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# 데이터셋 로드
dataset = load_dataset("json", data_files="train.jsonl")

# 전처리 함수
def format_example(example):
    inputs = tokenizer(
        example["instruction"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    labels = tokenizer(
        example["output"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    return {
        "input_ids": inputs["input_ids"][0],
        "labels": labels["input_ids"][0]
    }

tokenized_ds = dataset["train"].map(format_example)
dataloader = DataLoader(tokenized_ds, batch_size=1)

# 옵티마이저 설정
optimizer = AdamW(model.parameters(), lr=1e-4)

# 학습 루프
model.train()
for epoch in range(1):  # 빠른 테스트용
    for batch in tqdm(dataloader):
        input_ids = batch["input_ids"].unsqueeze(0).to(model.device)
        labels = batch["labels"].unsqueeze(0).to(model.device)

        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# LoRA 어댑터 저장
model.save_pretrained("./lora-llama3")
tokenizer.save_pretrained("./lora-llama3")
```
device_map 오류 -> .to("cuda")
```
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm

# 모델 경로
model_id = "./Meta-Llama-3-8B-Instruct"

# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# 모델 로드 (자동 분산 + 메모리 최적화)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA 설정
peft_config = LoraConfig(
    r=4,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# LoRA 적용
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# 데이터셋 로드
dataset = load_dataset("json", data_files="train.jsonl")

# 전처리 함수
def format_example(example):
    inputs = tokenizer(
        example["instruction"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    labels = tokenizer(
        example["output"],
        truncation=True,
        padding="max_length",
        max_length=2048,
        return_tensors="pt"
    )
    return {
        "input_ids": inputs["input_ids"][0],
        "labels": labels["input_ids"][0]
    }

tokenized_ds = dataset["train"].map(format_example)
dataloader = DataLoader(tokenized_ds, batch_size=1)

# 옵티마이저 설정
optimizer = AdamW(model.parameters(), lr=1e-4)

# 학습 루프
model.train()
for epoch in range(1):  # 빠른 테스트용
    for batch in tqdm(dataloader):
        input_ids = batch["input_ids"].unsqueeze(0).to(model.device)
        labels = batch["labels"].unsqueeze(0).to(model.device)

        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# LoRA 어댑터 저장
model.save_pretrained("./lora-llama3")
tokenizer.save_pretrained("./lora-llama3")
```

### Merge

```
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# base 모델 로드
base_model = AutoModelForCausalLM.from_pretrained(
    "./Meta-Llama-3-8B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA 어댑터 로드
lora_model = PeftModel.from_pretrained(base_model, "./lora-llama3")

# 병합
merged_model = lora_model.merge_and_unload()

# 저장
merged_model.save_pretrained("./merged-llama3", safe_serialization=False)
tokenizer = AutoTokenizer.from_pretrained("./Meta-Llama-3-8B-Instruct")
tokenizer.save_pretrained("./merged-llama3")
```

### Ollama model
```
echo 'FROM ./gemma-2-9b-it-Q6_K_L.gguf' > Modelfile
ollama create gemma-2-9b-it -f Modelfile
ollama list

ollama server
ollama run gemma-2-9b-it
```
