---
title: llama.cpp build
author: Raymond
date: 2025-07-20
category: Jekyll
layout: post
---
## Why ?
ChatGPT, Copilot 등의 상용은 상당한 수준이지만, 보안 문제 특히 내부 정보가 포함된 경우에는 사용하기 마음 놓고 사용하기 그러하다.  아무리 RAG 가 있다고는 하지만 ..<br>

HuggingFace 등을 통한 모델의 파인튜닝, 즉 LoRA 와 같은 것들을 하고 이를 ollama 등으로 변환하기위해서는 llama.cpp 가 필수적으로 필요하다는 것이 개인적인 생각이다.<br>

GPU 시스템을 모두 갖춰두기 어려워서 개인의 경우 Nvidia RTX 3060 (12GB)로 시작하는 것이 좋다고 여긴다. 이를 WSL 기반으로 Ubuntu 환경에서 빌드를 하면 어떨까 ? 한다. <br>

ollama 가 편하기 때문에 양자화를 거쳐서 ollama 모델로 생성하면 Open Web UI 등으로 좀더 편하게 작업할 수 있고 RAG 부분을 LoRA 로 가끔 파인튜닝하는 것을 목표로 한다. <br>

## Ubuntu 22.04 LTS

## Preparation
```
### ubuntu update error
sudo rm -rf /var/lib/apt/lists/*
sudo apt clean
sudo apt update
sudo apt upgrade

### ubuntu update ok
sudo apt update && sudo apt upgrade -y

### dependencies
sudo apt install -y build-essential cmake git libcurl4-openssl-dev

### case1) NVIDIA 12.4
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-4
   
### case2) NVIDIA 11.x - don't do this.
sudo apt install -y nvidia-cuda-toolkit

### NVIDIA cuda archive download and install
mkdir repos
# this is for WSL based.
explorer.exe .
### download or copy from your repo.
# cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz into repo directory

tar -xvf cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz
cd cudnn-linux-x86_64-8.9.7.29_cuda12-archive
sudo cp include/* /usr/local/cuda/include
sudo cp lib/* /usr/local/cuda/lib64

.bashrc
export PATH=/usr/local/cuda/bin:$PATH
source ~/.bashrc

nvcc --version # 12.4.x
nvidia-smi

```


## LLama build
```
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build && cd build
cmake .. -DGGML_CUDA=ON -DCMAKE_CUDA_STANDARD=17
cmake --build . --config Release -j$(nproc)

cd ..
cd models
wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
cd ..

```

## RUN LLama
```
./build/bin/llama-server   -m ./models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf   --port 8080   --host 0.0.0.0   --threads $(nproc)   --n-gpu-layers 35
```

`````
